import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,b as e,a as i,e as p,o as l}from"./app-C-Y0mTYs.js";const n={};function s(d,a){return l(),r("div",null,[a[0]||(a[0]=e("p",null,"Spark具有如下几个主要特点:",-1)),a[1]||(a[1]=e("ul",null,[e("li",null,"运行速度快：使用DAG执行引擎以支持循环数据流与内存计算"),e("li",null,"容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程"),e("li",null,"通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件"),e("li",null,"运行模式多样：可运行于独立的集群模式中，可运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源")],-1)),i(" more "),a[2]||(a[2]=p('<h2 id="spark简介" tabindex="-1"><a class="header-anchor" href="#spark简介"><span>Spark简介</span></a></h2><h3 id="spark具有如下几个主要特点" tabindex="-1"><a class="header-anchor" href="#spark具有如下几个主要特点"><span>Spark具有如下几个主要特点</span></a></h3><ul><li>运行速度快：使用DAG执行引擎以支持循环数据流与内存计算</li><li>容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程</li><li>通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件</li><li>运行模式多样：可运行于独立的集群模式中，可运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源</li></ul><h3 id="scala简介" tabindex="-1"><a class="header-anchor" href="#scala简介"><span>Scala简介</span></a></h3><ul><li><p>Scala具备强大的并发性，支持函数式编程，可以更好地支持分布式系统</p></li><li><p>Scala语法简洁，能提供优雅的API</p></li><li><p>Scala兼容Java，运行速度快，且能融合到Hadoop生态圈中</p></li></ul><h3 id="spark与hadoop的对比" tabindex="-1"><a class="header-anchor" href="#spark与hadoop的对比"><span>Spark与Hadoop的对比</span></a></h3><ul><li><p>Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活</p></li><li><p>Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高</p></li><li><p>Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制</p></li></ul><h2 id="spark生态系统" tabindex="-1"><a class="header-anchor" href="#spark生态系统"><span>Spark生态系统</span></a></h2><p>Spark的设计遵循“一个软件栈满足不同应用场景”的理念，逐渐形成了一套完整的生态系统，既能够提供内存计算框架，也可以支持SQL即席查询、实时流式计算、机器学习和图计算等。Spark可以部署在资源管理器YARN之上，提供一站式的大数据解决方案。因此，Spark所提供的生态系统足以应对上述三种场景，即同时支持批处理、交互式查询和流数据处理。</p><p>Spark的生态系统主要包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX 等组件。</p><p><strong>Spark生态系统组件的应用场景：</strong></p><table><thead><tr><th>应用场景</th><th>时间跨度</th><th>其他框架</th><th style="text-align:center;">Spark生态系统中的组件</th></tr></thead><tbody><tr><td>复杂的批量数据处理</td><td>小时级</td><td>MapReduce、Hive</td><td style="text-align:center;">Spark</td></tr><tr><td>基于历史数据的交互式查询</td><td>分钟级、秒级</td><td>Impala、Dremel、Drill</td><td style="text-align:center;">Spark SQL</td></tr><tr><td>基于实时数据流的数据处理</td><td>毫秒、秒级</td><td>Storm、S4</td><td style="text-align:center;">Spark Streaming</td></tr><tr><td>基于历史数据的数据挖掘</td><td>-</td><td>Mahout</td><td style="text-align:center;">MLlib</td></tr><tr><td>图结构数据的处理</td><td>-</td><td>Pregel、Hama</td><td style="text-align:center;">GraphX</td></tr></tbody></table><h2 id="spark运行架构" tabindex="-1"><a class="header-anchor" href="#spark运行架构"><span>Spark运行架构</span></a></h2><h3 id="基本概念" tabindex="-1"><a class="header-anchor" href="#基本概念"><span>基本概念</span></a></h3><ul><li>RDD：是Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型</li><li>DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系</li><li>Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task</li><li>Application：用户编写的Spark应用程序</li><li>Task：运行在Executor上的工作单元</li><li>Job：一个Job包含多个RDD及作用于相应RDD上的各种操作</li><li>Stage：是Job的基本调度单位，一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集</li></ul><h3 id="架构设计" tabindex="-1"><a class="header-anchor" href="#架构设计"><span>架构设计</span></a></h3><ul><li>Spark运行架构包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor）</li></ul><figure><img src="https://user-images.githubusercontent.com/22270117/60343264-2a236000-99e6-11e9-9cd9-1d7dcefcdb2e.png" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><ul><li>一个Application由一个Driver和若干个Job构成，一个Job由多个Stage构成，一个Stage由多个没有Shuffle关系的Task组成</li><li>当执行一个Application时，Driver会向集群管理器申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行Task，运行结束后，执行结果会返回给Driver，或者写到HDFS或者其他数据库中</li></ul><figure><img src="https://user-images.githubusercontent.com/22270117/60343283-390a1280-99e6-11e9-87b6-c1c93af7d8da.png" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h3 id="spark运行基本流程" tabindex="-1"><a class="header-anchor" href="#spark运行基本流程"><span>Spark运行基本流程</span></a></h3><ul><li>首先为应用构建起基本的运行环境，即由Driver创建一个SparkContext，进行资源的申请、任务的分配和监控</li><li>资源管理器为Executor分配资源，并启动Executor进程</li><li>SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler解析成Stage，然后把一个个TaskSet提交给底层调度器TaskScheduler处理；Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，并提供应用程序代码</li><li>Task在Executor上运行，把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源</li></ul><h3 id="rdd运行原理" tabindex="-1"><a class="header-anchor" href="#rdd运行原理"><span>RDD运行原理</span></a></h3><h4 id="rdd概念" tabindex="-1"><a class="header-anchor" href="#rdd概念"><span>RDD概念</span></a></h4><ul><li><p>一个RDD就是一个分布式对象集合，本质上是一个<strong>只读</strong>的分区记录集合，每个RDD可分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算</p></li><li><p>RDD提供了一种高度<strong>受限</strong>的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和group by）而创建得到新的RDD</p></li><li><p>RDD提供了一组丰富的操作以支持常见的数据运算，分为“动作”（Action）和“转换”（Transformation）两种类型</p></li><li><p>RDD提供的转换接口都非常简单，都是类似<strong>map、filter、groupBy、join</strong>等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改（<strong>不适合网页爬虫</strong>）</p></li><li><p>表面上RDD的功能很受限、不够强大，实际上RDD已经被实践证明可以高效地表达许多框架的编程模型（比如MapReduce、SQL、Pregel）</p></li><li><p>Spark用Scala语言实现了RDD的API，程序员可以通过调用API实现对RDD的各种操作</p></li></ul><p><strong>RDD典型的执行过程如下：</strong></p><ul><li><p>RDD读入外部数据源进行创建</p></li><li><p>RDD经过一系列的转换（Transformation）操作，每一次都会产生不同的RDD，供给下一个转换操作使用</p></li><li><p>最后一个RDD经过“动作”操作进行转换，并输出到外部数据源</p></li></ul><p>这一系列处理称为一个Lineage（血缘关系），即DAG拓扑排序的结果。</p><p>优点：惰性调用、管道化、避免同步等待、不需要保存中间结果、每次操作变得简单</p><h4 id="rdd特性" tabindex="-1"><a class="header-anchor" href="#rdd特性"><span>RDD特性</span></a></h4><ul><li>高效的容错性。RDD: 血缘关系、重新计算丢失分区、无需回滚系统、重算过程在不同节点之间并行、只记录粗粒度的操作</li><li>中间结果持久化到内存，数据在内存中的多个RDD操作之间进行传递，避免了不必要的读写磁盘开销</li><li>存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化</li></ul><h4 id="rdd的依赖关系" tabindex="-1"><a class="header-anchor" href="#rdd的依赖关系"><span>RDD的依赖关系</span></a></h4><figure><img src="https://user-images.githubusercontent.com/22270117/60343310-44f5d480-99e6-11e9-91a8-3c58331bc461.png" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><ul><li><p>窄依赖表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区</p></li><li><p>宽依赖则表现为存在一个父RDD的一个分区对应一个子RDD的多个分区</p></li></ul><h4 id="stage的划分" tabindex="-1"><a class="header-anchor" href="#stage的划分"><span>Stage的划分</span></a></h4><p>Spark通过分析各个RDD的依赖关系生成了DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage，具体划分方法是：</p><ul><li><p>在DAG中进行反向解析，遇到宽依赖就断开</p></li><li><p>遇到窄依赖就把当前的RDD加入到Stage中</p></li><li><p>将窄依赖尽量划分在同一个Stage中，可以实现流水线计算</p></li></ul><figure><img src="https://user-images.githubusercontent.com/22270117/60343349-5f2fb280-99e6-11e9-933d-8b1d6a580c8b.png" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h3 id="spark-sql" tabindex="-1"><a class="header-anchor" href="#spark-sql"><span>Spark SQL</span></a></h3><h4 id="shark" tabindex="-1"><a class="header-anchor" href="#shark"><span>Shark</span></a></h4><ul><li><p>Shark即Hive on Spark，为了实现与Hive兼容，Shark在HiveQL方面重用了Hive中HiveQL的解析、逻辑执行计划翻译、执行计划优化等逻辑，可以近似认为仅将物理执行计划从MapReduce作业替换成了Spark作业，通过Hive的HiveQL解析，把HiveQL翻译成Spark上的RDD操作。</p></li><li><p>Shark的设计导致了两个问题：</p><ul><li>一是执行计划优化完全依赖于Hive，不方便添加新的优化策略；</li><li>二是因为Spark是线程级并行，而MapReduce是进程级并行，因此，Spark在兼容Hive的实现上存在线程安全问题，导致Shark不得不使用另外一套独立维护的打了补丁的Hive源码分支</li></ul></li></ul><h4 id="spark-sql设计" tabindex="-1"><a class="header-anchor" href="#spark-sql设计"><span>Spark SQL设计</span></a></h4><p>Spark SQL在Hive兼容层面仅依赖HiveQL解析、Hive元数据，也就是说，从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。Spark SQL执行计划生成和优化都由Catalyst（函数式关系查询优化框架）负责。</p><ul><li><p>Spark SQL增加了SchemaRDD（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据</p></li><li><p>Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范</p></li></ul><h2 id="spark的部署和应用方式" tabindex="-1"><a class="header-anchor" href="#spark的部署和应用方式"><span>Spark的部署和应用方式</span></a></h2><h3 id="spark应用程序" tabindex="-1"><a class="header-anchor" href="#spark应用程序"><span>Spark应用程序</span></a></h3><ul><li>Python</li><li>Scala</li></ul>',47))])}const h=t(n,[["render",s]]),S=JSON.parse('{"path":"/readings/spark/first%20acquaintance%20with%20Spark.html","title":"Spark系列: 初识Spark","lang":"zh-CN","frontmatter":{"title":"Spark系列: 初识Spark","icon":"pen-to-square","date":"2019-01-18T00:00:00.000Z","category":["大数据"],"tag":["spark","入门"],"description":"Spark具有如下几个主要特点: 运行速度快：使用DAG执行引擎以支持循环数据流与内存计算 容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程 通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件 运行模式多样：可运行于独立的集群模式中，可运行于Had...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Spark系列: 初识Spark\\",\\"image\\":[\\"https://user-images.githubusercontent.com/22270117/60343264-2a236000-99e6-11e9-9cd9-1d7dcefcdb2e.png\\",\\"https://user-images.githubusercontent.com/22270117/60343283-390a1280-99e6-11e9-87b6-c1c93af7d8da.png\\",\\"https://user-images.githubusercontent.com/22270117/60343310-44f5d480-99e6-11e9-91a8-3c58331bc461.png\\",\\"https://user-images.githubusercontent.com/22270117/60343349-5f2fb280-99e6-11e9-933d-8b1d6a580c8b.png\\"],\\"datePublished\\":\\"2019-01-18T00:00:00.000Z\\",\\"dateModified\\":\\"2024-04-09T13:47:09.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"shilinlee\\",\\"url\\":\\"https://shilinlee.com\\"}]}"],["meta",{"property":"og:url","content":"https://shilinlee.com/readings/spark/first%20acquaintance%20with%20Spark.html"}],["meta",{"property":"og:site_name","content":"shilinlee的博客"}],["meta",{"property":"og:title","content":"Spark系列: 初识Spark"}],["meta",{"property":"og:description","content":"Spark具有如下几个主要特点: 运行速度快：使用DAG执行引擎以支持循环数据流与内存计算 容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程 通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件 运行模式多样：可运行于独立的集群模式中，可运行于Had..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://user-images.githubusercontent.com/22270117/60343264-2a236000-99e6-11e9-9cd9-1d7dcefcdb2e.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-04-09T13:47:09.000Z"}],["meta",{"property":"article:tag","content":"入门"}],["meta",{"property":"article:tag","content":"spark"}],["meta",{"property":"article:published_time","content":"2019-01-18T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-04-09T13:47:09.000Z"}]]},"git":{"createdTime":1712450729000,"updatedTime":1712670429000,"contributors":[{"name":"shilinlee","username":"shilinlee","email":"836160610@qq.com","commits":2,"url":"https://github.com/shilinlee"}]},"readingTime":{"minutes":8.4,"words":2519},"filePathRelative":"readings/spark/first acquaintance with Spark.md","excerpt":"<p>Spark具有如下几个主要特点:</p>\\n<ul>\\n<li>运行速度快：使用DAG执行引擎以支持循环数据流与内存计算</li>\\n<li>容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程</li>\\n<li>通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件</li>\\n<li>运行模式多样：可运行于独立的集群模式中，可运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源</li>\\n</ul>\\n","autoDesc":true}');export{h as comp,S as data};
